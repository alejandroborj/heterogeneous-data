{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import dataset\n",
    "import data_reader\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU : cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Choosing device for tensor processing\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU :\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"tcia_25ep\" #\"25ep0\"#\n",
    "\n",
    "DATA_SET_NAME = \"tcia_data_set_SPLIT\" #f\"data_set_X20_100%_SPLIT\"#\n",
    "\n",
    "N_PATCHES = 400 # Number of patches to take from each WSI\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m): # XAVIER initialization for final layer weight initialization\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def init():\n",
    "    global model, loss_function, optimizer\n",
    "\n",
    "    # Final dense layer classifier for WSI\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(N_PATCHES, 2),\n",
    "        #nn.ReLU(inplace=True),\n",
    "        #nn.Linear(256,2),\n",
    "        nn.Softmax(dim=-1)\n",
    "    ).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.apply(init_weights) # Xavier init\n",
    "    \n",
    "    #Hyperparameters:\n",
    "    learning_rate = 1E-3 # LR\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)#, betas=(0.9, 0.999), eps=1e-08)# Optimizer\n",
    "\n",
    "    loss_function = nn.BCELoss()  # Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_feature_extraction(wsi_data_set):\n",
    "    global net\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - wsi_data_set, patches from a given WSI, dataset object\n",
    "    - y, patch labels\n",
    "    Outputs:\n",
    "    - output: features of the selected N_PATCHES patches\n",
    "    \"\"\"\n",
    "    wsi_dataloader = DataLoader(wsi_data_set, batch_size=1)\n",
    "    outputs = []\n",
    "\n",
    "    for batch_X, batch_y in iter(wsi_dataloader):\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1), batch_y.type(torch.FloatTensor).to(device) \n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "        for i, x in enumerate(batch_X):\n",
    "            batch_X[i] = normalize(batch_X[i]/255) # NP array\n",
    "    \n",
    "        output = net(batch_X)[0]\n",
    "        outputs.append(output)\n",
    "\n",
    "    outputs = torch.tensor(outputs).to(device)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(wsi_data_set, train=False):\n",
    "\n",
    "    X = torch.tensor(wsi_data_set.inputs).to(device)\n",
    "    y = torch.tensor(wsi_data_set.labels[0][0],dtype=torch.float).to(device)\n",
    "\n",
    "    if train: \n",
    "        model.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = patch_feature_extraction(wsi_data_set)\n",
    "\n",
    "    output = model(features)\n",
    "    #print(output[0], y)\n",
    "    \n",
    "    y_pred = [torch.argmax(output)]\n",
    "    y_true = [torch.argmax(y)]\n",
    "\n",
    "    loss = loss_function(output[0], y)\n",
    "    #print(loss)\n",
    "    #loss.requires_grad = True\n",
    "\n",
    "    if train:\n",
    "        loss.backward() # Calculate gradients using backprop\n",
    "        optimizer.step() # Updates W and b using previously calculated gradients\n",
    "\n",
    "    return [loss, y_pred, y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcia_data_set_SPLIT1\n",
      "Loading training split: 2\n",
      "Read lmdb\n",
      "20366\n",
      "Loading training split: 3\n",
      "Read lmdb\n",
      "20818\n",
      "Loading training split: 4\n",
      "Read lmdb\n",
      "20947\n",
      "Loading training split: 5\n",
      "Read lmdb\n",
      "26374\n",
      "Loading training split: 6\n",
      "Read lmdb\n",
      "14995\n",
      "Loading training split: 7\n",
      "Read lmdb\n",
      "10407\n",
      "Loading training split: 8\n",
      "Read lmdb\n",
      "22786\n",
      "Loading training split: 9\n",
      "Read lmdb\n",
      "19800\n",
      "EPOCH:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868c5fd49ddc46f6baf8f5aafdebda1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_3272\\1371018220.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  X = torch.tensor(wsi_data_set.inputs).to(device)\n",
      "c:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:146: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No patches\n",
      "No patches\n",
      "No patches\n",
      "No patches\n",
      "No patches\n",
      "No patches\n",
      "No patches\n",
      "No patches\n",
      "No patches\n"
     ]
    }
   ],
   "source": [
    "n_splits = 10\n",
    "for SPLIT in range(n_splits):\n",
    "    \n",
    "    X, y, case_ids = [], [], []\n",
    "\n",
    "    init() # Initializing patch merging model\n",
    "        \n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    TRAIN_SPLITS = list(range(n_splits))\n",
    "    # We take out the SPLIT and SPLIT+1 sets for val and testing\n",
    "    if SPLIT == n_splits-1: # For the final split for validation we take the first one for test\n",
    "        TRAIN_SPLITS.remove(0) \n",
    "    else:\n",
    "        TRAIN_SPLITS.remove(SPLIT+1)\n",
    "    TRAIN_SPLITS.remove(SPLIT)\n",
    "    \n",
    "    if SPLIT == n_splits-1: # For the final split for validation we take the firs one for test\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{0}\" # Test set \n",
    "    else:\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT+1}\" # Test set\n",
    "    \n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    print(SPLIT_NAME)\n",
    "\n",
    "    net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "    net.eval()\n",
    "    net.fc = nn.Sequential(nn.AvgPool1d(kernel_size=512))\n",
    "\n",
    "    # Loading training splits:\n",
    "    for i in TRAIN_SPLITS:\n",
    "        print(f\"Loading training split: {i}\")\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{i}\"\n",
    "        \n",
    "        X_, y_, _ , patch_ids_ = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "        X.extend(X_)\n",
    "        y.extend(y_)\n",
    "        case_ids.extend(patch_ids_)\n",
    "\n",
    "    # Loading validation splits:\n",
    "    SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    print(f\"Loading validation split: {SPLIT}\")\n",
    "\n",
    "    val_X, val_y, _ , val_case_ids = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "\n",
    "    sample_ids = [case_id.split(\"_\")[1] for case_id in case_ids] # Taking only the sample_id, not patch_id\n",
    "    val_sample_ids = [case_id.split(\"_\")[1] for case_id in val_case_ids]\n",
    "\n",
    "    unique_sample_ids = np.unique(sample_ids)\n",
    "    val_unique_sample_ids = np.unique(sample_ids)\n",
    "    random.shuffle(unique_sample_ids)\n",
    "    random.shuffle(val_unique_sample_ids)\n",
    "    sample_ids = np.array(sample_ids)\n",
    "    val_sample_ids = np.array(val_sample_ids)\n",
    "\n",
    "    for EPOCH in range(EPOCHS):\n",
    "        print(\"EPOCH: \", EPOCH+1)\n",
    "        losses, outputs, labels = [], [], []\n",
    "        val_losses, val_outputs, val_labels = [], [], []\n",
    "\n",
    "        for unique_sample_id in tqdm(unique_sample_ids):\n",
    "\n",
    "            ii = np.where(sample_ids == unique_sample_id)[0]\n",
    "\n",
    "            wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "            feature_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "            wsi_data_set.inputs.extend(X[ii[0]:ii[-1]][:N_PATCHES])# Taking the patches from a given wsi\n",
    "            wsi_data_set.labels.extend(y[ii[0]:ii[-1]][:N_PATCHES])\n",
    "            wsi_data_set.case_ids.extend(sample_ids[:N_PATCHES])\n",
    "\n",
    "            if len(wsi_data_set) < N_PATCHES:\n",
    "                print(\"No patches\")\n",
    "            else:\n",
    "                model.train()\n",
    "                output = fwd_pass(wsi_data_set, train=True)\n",
    "                losses.append(float(output[0]))\n",
    "                outputs.append(output[1][0].cpu())\n",
    "                labels.append(output[2][0].cpu())\n",
    "\n",
    "            del wsi_data_set\n",
    "        \n",
    "        for unique_sample_id in tqdm(val_unique_sample_ids):\n",
    "\n",
    "            ii = np.where(sample_ids == unique_sample_id)[0]\n",
    "\n",
    "            wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "            feature_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "            wsi_data_set.inputs.extend(X[ii[0]:ii[-1]][:N_PATCHES])# Taking the patches from a given wsi\n",
    "            wsi_data_set.labels.extend(y[ii[0]:ii[-1]][:N_PATCHES])\n",
    "            wsi_data_set.case_ids.extend(sample_ids[:N_PATCHES])\n",
    "\n",
    "            if len(wsi_data_set) < N_PATCHES:\n",
    "                print(\"No patches\")\n",
    "            else:\n",
    "                model.train()\n",
    "                output = fwd_pass(wsi_data_set, train=False)\n",
    "                val_losses.append(float(output[0]))\n",
    "                val_outputs.append(output[1][0].cpu())\n",
    "                val_labels.append(output[2][0].cpu())\n",
    "\n",
    "            del wsi_data_set\n",
    "                \n",
    "        train_loss = np.mean(losses)\n",
    "        train_acc = sklearn.metrics.accuracy_score(labels, outputs)\n",
    "        train_bacc = sklearn.metrics.balanced_accuracy_score(labels,outputs)\n",
    "\n",
    "        val_loss = np.mean(losses)\n",
    "        val_acc = sklearn.metrics.accuracy_score(labels, outputs)\n",
    "        val_bacc = sklearn.metrics.balanced_accuracy_score(labels,outputs)\n",
    "\n",
    "        print(\"Train Loss: \", train_loss, \"Train ACC:\", train_acc, \"Train BACC\", train_bacc)\n",
    "\n",
    "        print(\"Val Loss: \", val_loss, \"Val ACC:\", val_acc, \"Val BACC\", val_bacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openslide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de74446a7cd3a56c425ecbffb2a7ad915342ddab3c48353acb4566475bd7705f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
