{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import dataset\n",
    "import data_reader\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing device for tensor processing\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU :\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"tcia_25ep\" #\"25ep0\"#\n",
    "\n",
    "DATA_SET_NAME = \"tcia_data_set_SPLIT\" #f\"data_set_X20_100%_SPLIT\"#\n",
    "\n",
    "N_PATCHES = 400 # Number of patches to take from each WSI\n",
    "\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m): # XAVIER initialization for final layer weight initialization\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def init():\n",
    "    global model, loss_function, optimizer\n",
    "\n",
    "    # Final dense layer classifier for WSI\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(512, 2),\n",
    "        nn.Dropout(0.5),\n",
    "        #nn.ReLU(inplace=True),\n",
    "        #nn.Linear(256,2),\n",
    "        nn.Softmax(dim=-1)\n",
    "    ).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.apply(init_weights) # Xavier init\n",
    "    \n",
    "    #Hyperparameters:\n",
    "    learning_rate = 1E-4 # 1E-4 # LR\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)#, betas=(0.9, 0.999), eps=1e-08)# Optimizer\n",
    "\n",
    "    loss_function = nn.BCELoss()  # Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_feature_extraction(wsi_data_set):\n",
    "    global net\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - wsi_data_set, patches from a given WSI, dataset object\n",
    "    - y, patch labels\n",
    "    Outputs:\n",
    "    - output: features of the selected N_PATCHES patches\n",
    "    \"\"\"\n",
    "    wsi_dataloader = DataLoader(wsi_data_set, batch_size=1)\n",
    "    outputs = []\n",
    "\n",
    "            \n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    for batch_X, batch_y, patch_ids in iter(wsi_dataloader):\n",
    "\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1), batch_y.type(torch.FloatTensor).to(device) \n",
    "\n",
    "        for i, x in enumerate(batch_X):\n",
    "            batch_X[i] = normalize(batch_X[i]/255)\n",
    "        \n",
    "        output = list(net(batch_X)[0].cpu())\n",
    "        outputs.append(output)\n",
    "\n",
    "    return torch.tensor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(wsi_data_set, train=False):\n",
    "    global feature_dataset\n",
    "\n",
    "    features = []\n",
    "    y = torch.tensor(wsi_data_set.labels[0], dtype=torch.float).to(device)\n",
    "    \n",
    "    if train: \n",
    "        model.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        if wsi_data_set.case_ids[0] in feature_dataset.keys(): # Using saved computations\n",
    "            features = feature_dataset[wsi_data_set.case_ids[0]]\n",
    "            #print(\"Using saved computations\")\n",
    "        else:\n",
    "            features = torch.mean(patch_feature_extraction(wsi_data_set), dim=0).to(device)\n",
    "            feature_dataset[wsi_data_set.case_ids[0]] = (features)\n",
    "    #print(features)\n",
    "    output = model(features)\n",
    "    #print(output[0], y\n",
    "    \n",
    "    y_pred = [torch.argmax(output)]\n",
    "    y_true = [torch.argmax(y)]\n",
    "\n",
    "    loss = loss_function(output[0], y[0])\n",
    "    #print(output[0], y[0])\n",
    "    #loss.requires_grad = True\n",
    "\n",
    "    if train:\n",
    "        loss.backward() # Calculate gradients using backprop\n",
    "        optimizer.step() # Updates W and b using previously calculated gradients\n",
    "\n",
    "    return [loss, y_pred, y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "feature_dataset = dict()\n",
    "\n",
    "for SPLIT in range(10):\n",
    "    ep_loss, val_ep_loss = [], []\n",
    "    ep_acc, val_ep_acc = [], []\n",
    "    \n",
    "    X, y, case_ids = [], [], []\n",
    "        \n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "    \n",
    "    TRAIN_SPLITS = list(range(n_splits))\n",
    "    # We take out the SPLIT and SPLIT+1 sets for val and testing\n",
    "    if SPLIT == n_splits-1: # For the final split for validation we take the first one for test\n",
    "        TRAIN_SPLITS.remove(0) \n",
    "    else:\n",
    "        TRAIN_SPLITS.remove(SPLIT+1)\n",
    "    TRAIN_SPLITS.remove(SPLIT)\n",
    "    \n",
    "    if SPLIT == n_splits-1: # For the final split for validation we take the firs one for test\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{0}\" # Test set \n",
    "    else:\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT+1}\" # Test set\n",
    "    \n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    print(SPLIT_NAME)\n",
    "\n",
    "    init() # Initializing patch merging model\n",
    "\n",
    "    net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "    net.eval()\n",
    "    net.fc = nn.Identity()#nn.Sequential(nn.AvgPool1d(kernel_size=512))\n",
    "\n",
    "    # Loading validation splits:\n",
    "    SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    print(f\"Loading validation split: {SPLIT}\")\n",
    "\n",
    "    val_X, val_y, _ , val_patch_ids = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "    # Loading training splits:\n",
    "    for i in TRAIN_SPLITS:\n",
    "        print(f\"Loading training split: {i}\")\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{i}\"\n",
    "        \n",
    "        X_, y_, _ , patch_ids_ = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "        X.extend(X_)\n",
    "        y.extend(y_)\n",
    "        case_ids.extend(patch_ids_)\n",
    "\n",
    "    sample_ids = [case_id.split(\"_\")[1] for case_id in case_ids] # Taking only the sample_id, not patch_id\n",
    "    val_sample_ids = [case_id.split(\"_\")[1] for case_id in val_patch_ids]\n",
    "\n",
    "    unique_sample_ids = np.unique(sample_ids)\n",
    "    val_unique_sample_ids = np.unique(val_sample_ids)\n",
    "    random.shuffle(unique_sample_ids)\n",
    "    random.shuffle(val_unique_sample_ids)\n",
    "    sample_ids = np.array(sample_ids)\n",
    "    val_sample_ids = np.array(val_sample_ids)\n",
    "\n",
    "    for EPOCH in range(EPOCHS):\n",
    "        print(\"EPOCH: \", EPOCH+1)\n",
    "        losses, accs, outputs, labels = [], [], [], []\n",
    "        val_losses, val_accs, val_outputs, val_labels = [], [], [], []\n",
    "        \n",
    "        for unique_sample_id in tqdm(unique_sample_ids):\n",
    "\n",
    "            ii = np.where(sample_ids == unique_sample_id)[0]\n",
    "\n",
    "            wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "            #feature_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "            wsi_data_set.inputs.extend(X[ii[0]:ii[-1]][:N_PATCHES])# Taking the patches from a given wsi\n",
    "            wsi_data_set.labels.extend(y[ii[0]:ii[-1]][:N_PATCHES])\n",
    "            wsi_data_set.case_ids.extend(sample_ids[ii[0]:ii[-1]][:N_PATCHES])\n",
    "            prob = np.random.uniform()\n",
    "\n",
    "            if len(wsi_data_set) < N_PATCHES:\n",
    "                a = 1\n",
    "                # print(\"No patches\")\n",
    "            elif prob<0.5 and wsi_data_set.labels[0][0]==0 or wsi_data_set.labels[0][0]==1: # Undersampling\n",
    "                model.train()\n",
    "                output = fwd_pass(wsi_data_set, train=True)\n",
    "                losses.append(float(output[0]))\n",
    "                outputs.append(output[1][0].cpu())\n",
    "                labels.append(output[2][0].cpu())\n",
    "\n",
    "            del wsi_data_set\n",
    "        \n",
    "        for unique_sample_id in tqdm(val_unique_sample_ids):\n",
    "\n",
    "            ii = np.where(val_sample_ids == unique_sample_id)[0]\n",
    "            \n",
    "            wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "            #feature_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "            wsi_data_set.inputs.extend(val_X[ii[0]:ii[-1]][:N_PATCHES])# Taking the patches from a given wsi\n",
    "            wsi_data_set.labels.extend(val_y[ii[0]:ii[-1]][:N_PATCHES])\n",
    "            wsi_data_set.case_ids.extend(val_patch_ids[ii[0]:ii[-1]][:N_PATCHES])\n",
    "\n",
    "            if len(wsi_data_set) < N_PATCHES:\n",
    "                a=1\n",
    "                #print(\"No patches\")\n",
    "            else:\n",
    "                model.eval()\n",
    "                output = fwd_pass(wsi_data_set, train=False)\n",
    "                val_losses.append(float(output[0]))\n",
    "                val_outputs.append(output[1][0].cpu())\n",
    "                val_labels.append(output[2][0].cpu())\n",
    "\n",
    "            del wsi_data_set\n",
    "                \n",
    "        train_loss = np.mean(losses)\n",
    "        train_acc = sklearn.metrics.accuracy_score(labels, outputs)\n",
    "        train_bacc = sklearn.metrics.balanced_accuracy_score(labels, outputs)\n",
    "        train_f1 = sklearn.metrics.f1_score(labels, outputs, average=\"macro\")   \n",
    "        train_conf_m = sklearn.metrics.confusion_matrix(labels, outputs, labels=[0, 1])\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = sklearn.metrics.accuracy_score(val_labels, val_outputs)\n",
    "        val_bacc = sklearn.metrics.balanced_accuracy_score(val_labels, val_outputs)\n",
    "        val_f1 = sklearn.metrics.f1_score(val_labels, val_outputs, average=\"macro\")\n",
    "        val_conf_m = sklearn.metrics.confusion_matrix(val_labels, val_outputs, labels=[0, 1])\n",
    "\n",
    "        print(\"Train Loss: \", train_loss, \"Train ACC:\", train_acc, \"Train F1:\", train_f1, \"\\nTrain CONF M:\\n\", train_conf_m)\n",
    "        print(\"Val Loss: \", val_loss, \"Val ACC:\", val_acc, \"Val F1:\", val_f1, \"\\nVal CONF M:\\n\", val_conf_m)\n",
    "\n",
    "        ep_loss.append(train_loss)\n",
    "        ep_acc.append(train_acc)\n",
    "\n",
    "        val_ep_loss.append(val_loss)\n",
    "        val_ep_acc.append(val_acc)\n",
    "        \n",
    "    MODEL_NAME = f\"feat_avg_tcia_{SPLIT}\"\n",
    "    torch.save(model, f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\"\n",
    "plt.plot(ep_loss)\n",
    "plt.plot(val_ep_loss)\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(ep_acc)\n",
    "plt.plot(val_ep_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_loss)\n",
    "plt.plot(val_ep_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = dict()\n",
    "\n",
    "init()\n",
    "\n",
    "test_acc, test_loss, test_f1 = [], [], []\n",
    "tot_conf = np.zeros((2,2))\n",
    "\n",
    "#DATA_SET_NAME = \n",
    "\n",
    "for SPLIT in range(10):\n",
    "    \n",
    "    X, y, case_ids = [], [], []\n",
    "\n",
    "    MODEL_NAME = f\"feat_avg_tcia_{SPLIT}\"\n",
    "    print(\"AVG model: \", MODEL_NAME)\n",
    "    model = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\")\n",
    "        \n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "    print(\"Feat extr model: \", MODEL_NAME)\n",
    "\n",
    "    net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "    net.eval()\n",
    "    net.fc = nn.Identity()#nn.Sequential(nn.AvgPool1d(kernel_size=512))\n",
    "    \n",
    "    if SPLIT==9:\n",
    "        TEST_SPLIT = 0\n",
    "    else:\n",
    "        TEST_SPLIT = SPLIT+1\n",
    "\n",
    "    # Loading test split:\n",
    "    SPLIT_NAME = DATA_SET_NAME + f\"{TEST_SPLIT}\"\n",
    "\n",
    "    print(f\"Loading test split: {TEST_SPLIT}\")\n",
    "\n",
    "    val_X, val_y, _ , val_patch_ids = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "    val_sample_ids = [case_id.split(\"_\")[1] for case_id in val_patch_ids]\n",
    "\n",
    "    val_unique_sample_ids = np.unique(val_sample_ids)\n",
    "    random.shuffle(val_unique_sample_ids)\n",
    "    val_sample_ids = np.array(val_sample_ids)\n",
    "\n",
    "    val_losses, val_accs, val_outputs, val_labels = [], [], [], []\n",
    "    \n",
    "    for unique_sample_id in tqdm(val_unique_sample_ids):\n",
    "\n",
    "        ii = np.where(val_sample_ids == unique_sample_id)[0]\n",
    "        \n",
    "        wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "        #feature_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "        wsi_data_set.inputs.extend(val_X[ii[0]:ii[-1]][:N_PATCHES])# Taking the patches from a given wsi\n",
    "        wsi_data_set.labels.extend(val_y[ii[0]:ii[-1]][:N_PATCHES])\n",
    "        wsi_data_set.case_ids.extend(val_patch_ids[ii[0]:ii[-1]][:N_PATCHES])\n",
    "\n",
    "        if len(wsi_data_set) < N_PATCHES:\n",
    "            a=1\n",
    "            #print(\"No patches\")\n",
    "        else:\n",
    "            model.eval()\n",
    "            output = fwd_pass(wsi_data_set, train=False)\n",
    "            val_losses.append(float(output[0]))\n",
    "            val_outputs.append(output[1][0].cpu())\n",
    "            val_labels.append(output[2][0].cpu())\n",
    "\n",
    "    del wsi_data_set\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = sklearn.metrics.accuracy_score(val_labels, val_outputs)\n",
    "    val_bacc = sklearn.metrics.balanced_accuracy_score(val_labels, val_outputs)\n",
    "    val_f1 = sklearn.metrics.f1_score(val_labels, val_outputs, average=\"macro\")\n",
    "    val_conf_m = sklearn.metrics.confusion_matrix(val_labels, val_outputs, labels=[0, 1])\n",
    "\n",
    "    print(\"Test Loss: \", val_loss, \"Test ACC:\", val_acc, \"Test F1:\", val_f1)\n",
    "    print(\"CONf: \", val_conf_m)\n",
    "\n",
    "    test_acc.append(val_acc)\n",
    "    test_loss.append(val_loss)\n",
    "    test_f1.append(val_f1)\n",
    "    tot_conf+=val_conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tot_conf)\n",
    "print(np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openslide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de74446a7cd3a56c425ecbffb2a7ad915342ddab3c48353acb4566475bd7705f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
