{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import time\n",
    "\n",
    "import data_reader\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU : cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Choosing device for tensor processing\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU :\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and data variables\n",
    "\n",
    "MODE = \"w\"\n",
    "EXP_NAME = \"10ep\"\n",
    "\n",
    "DATA_SET_NAME = f\"data_set_x20_1%_split\"\n",
    "PATCH_SIZE = 512\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "\n",
    "SIZE_X = PATCH_SIZE\n",
    "SIZE_Y = PATCH_SIZE\n",
    "\n",
    "threshold = 0.05 # Loss difference for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m): # XAVIER initialization for final layer weight initialization\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def init():\n",
    "    global net, loss_function, scheduler, optimizer, MODEL_NAME\n",
    "\n",
    "    net = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False # Freezing the convolutional layers\n",
    "\n",
    "    #for param in net.layer4[1].parameters():\n",
    "    #    param.requires_grad = True # Unfreezing the last residual block\n",
    "    \n",
    "    net.fc = nn.Sequential(\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(128, 3),\n",
    "                nn.Softmax(dim = -1)\n",
    "                ).to(device)\n",
    "\n",
    "    net.fc.apply(init_weights) # Xavier init\n",
    "\n",
    "    #print(f\"Loading {MODEL_NAME}\")\n",
    "    #net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "\n",
    "    n_params = sum(p.numel() for p in net.fc.parameters())# + sum(p.numel() for p in net.layer4[1].parameters())\n",
    "    print(\"Number of free parameters: \", n_params)\n",
    "\n",
    "    #Hyperparameters:\n",
    "    learning_rate = 1E-3 # LR\n",
    "    loss_function = nn.BCELoss()  # Loss # [1,0] es positivo y [0,1] negativo\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08)# Optimizer\n",
    "    lambda1 = lambda epoch: 1 ** epoch # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, train=False):\n",
    "# IMPORTANTE, TRAIN = FALSE PARA QUE NO ENTRENE CON EL TEST DATA ESTO ES PARA PODER HACER TEST MIENTRAS ENTRENO Y VALIDO, \n",
    "# SE ESPERA QUE LA EXACTITUD EN EL TEST DE VALIDACIÃ“N SEA MENOR\n",
    "    if train: \n",
    "        net.zero_grad()\n",
    "        \n",
    "    # NORMALIZATION\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    normalize = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = normalize(X[i]/255) # Np array\n",
    "\n",
    "    outputs = net(X)\n",
    "    #matches = [torch.argmax(i) == torch.argmax(j) for i,j in zip(outputs, y)]\n",
    "    y_pred = [torch.argmax(i) for i in outputs.cpu()]\n",
    "    y_true = [torch.argmax(i) for i in y.cpu()]\n",
    "\n",
    "    #acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "    conf_m = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
    "    acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred, average= \"micro\")\n",
    "    auc = 0 #sklearn.metrics.roc_auc_score(y_true, y_pred, average= \"micro\", multi_class=\"ovr\") #!!!!\n",
    "    \n",
    "    if train:\n",
    "        loss.backward() # Calculate gradients using backprop\n",
    "        optimizer.step() # Updates W and b using previously calculated gradients\n",
    "\n",
    "    return [acc, loss, conf_m, f1, auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  global net, loss_function, scheduler, threshold, optimizer, train_set, val_set, MODEL_NAME, EPOCHS\n",
    "  \n",
    "  print(MODEL_NAME)\n",
    "  patience = 0\n",
    "\n",
    "  with open(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\log\\\\model_{MODEL_NAME}.log\", MODE) as f:\n",
    "    for epoch in range(EPOCHS):\n",
    "      acc, loss, f1, auc = 0, 0, 0, 0\n",
    "      val_acc, val_loss, val_f1, val_auc = 0, 0, 0, 0\n",
    "      conf_m, val_conf_m = np.zeros((3,3)), np.zeros((3,3))\n",
    "\n",
    "      print(\"\\nEPOCH: \", epoch+1)\n",
    "\n",
    "      for batch_X, batch_y in tqdm(iter(train_dataloader)):\n",
    "\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1), batch_y.type(torch.FloatTensor).to(device) \n",
    "        \n",
    "        net.train() # Making sure that the model is in training mode\n",
    "        \n",
    "        performance_metrics = fwd_pass(batch_X, batch_y, train=True)\n",
    "        acc_aux, loss_aux, conf_m_aux, f1_aux, auc_aux = performance_metrics[0], performance_metrics[1], performance_metrics[2], performance_metrics[3], performance_metrics[4]\n",
    "        acc += acc_aux*(len(batch_X)/len(train_set)) # Calculating the average loss and acc through batches sum ACCi*Wi/N (Wi = weight of the batch)\n",
    "        loss += loss_aux*(len(batch_X)/len(train_set))\n",
    "        conf_m += conf_m_aux\n",
    "        f1 += f1_aux*(len(batch_X)/len(train_set))\n",
    "        auc += auc_aux*(len(batch_X)/len(train_set))\n",
    "        \"\"\"\n",
    "        i += 1\n",
    "    \n",
    "        if i%100 == 0:\n",
    "          print(\"Memory allocated in GPU: \", torch.cuda.memory_allocated(\"cuda:0\")/1024/1024/1024)\n",
    "        \"\"\"\n",
    "        \n",
    "      for batch_X, batch_y in tqdm(iter(val_dataloader)):\n",
    "\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1), batch_y.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        net.eval() # Making sure that the model is not training and deactivate droptout\n",
    "        \n",
    "        with torch.no_grad(): # Disable all computations, works together with net.eval()\n",
    "          performance_metrics = fwd_pass(batch_X, batch_y, train=False)\n",
    "          \n",
    "        acc_aux, loss_aux, conf_m_aux, f1_aux, auc_aux = performance_metrics[0], performance_metrics[1], performance_metrics[2], performance_metrics[3], performance_metrics[4]  \n",
    "        val_acc += acc_aux*(len(batch_X)/len(val_set)) # Calculating the average loss and acc through batches sum ACCi*Wi/N (Wi = weight of the batch)\n",
    "        val_loss += loss_aux*(len(batch_X)/len(val_set))\n",
    "        val_conf_m += conf_m_aux\n",
    "        val_f1 += f1_aux*(len(batch_X)/len(val_set))\n",
    "        val_auc += auc_aux*(len(batch_X)/len(val_set))\n",
    "      \n",
    "      print(\"Val loss: \", val_loss.item(),\" Train loss: \", loss.item(), \"\\n\")\n",
    "      print(\"Val acc: \", val_acc,\" Train acc: \", acc, \"\\n\")\n",
    "      print(\"Val AUC: \", val_auc,\"Train AUC: \", auc)\n",
    "      print(\"Val f1: \", val_f1,\" Train f1: \", f1, \"\\n\")\n",
    "      print(\"Val CONF: \\n\", val_conf_m,\"\\nTrain CONF: \\n\", conf_m, \"\\n\")\n",
    "\n",
    "      conf_m = f\"{conf_m[0][0]}+{conf_m[0][1]}+{conf_m[1][0]}+{conf_m[1][1]}\"\n",
    "      val_conf_m = f\"{val_conf_m[0][0]}+{val_conf_m[0][1]}+{val_conf_m[1][0]}+{val_conf_m[1][1]}\"\n",
    "    \n",
    "      f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),3)},{round(float(loss),4)},{conf_m},{round(float(auc),4)},\")\n",
    "      f.write(f\"{round(float(val_acc),3)},{round(float(val_loss),4)},{val_conf_m}, {round(float(val_auc),4)}\\n\")\n",
    "      f.write(\"\\n\\n\")\n",
    "\n",
    "      # Early stopping, if the difference between loss and validation loss \n",
    "      # is bigger than the threshold for 3 epochs in a row training is stopped\n",
    "      if loss.item()-val_loss.item()> threshold:\n",
    "        patience +=1\n",
    "      else:\n",
    "        patience = 0\n",
    "\n",
    "      print(\"Learning Rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "      scheduler.step() # Changing the learning rate\n",
    "\n",
    "      if patience >= 4:\n",
    "        print(\"Stopping early: \")\n",
    "        break\n",
    "\n",
    "    torch.save(net, f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training split: 2\n",
      "Read lmdb\n",
      "Loading training split: 3\n",
      "Read lmdb\n",
      "Loading training split: 4\n",
      "Read lmdb\n",
      "Loading training split: 5\n",
      "Read lmdb\n",
      "Loading training split: 6\n",
      "Read lmdb\n",
      "Loading training split: 7\n",
      "Read lmdb\n",
      "Loading training split: 8\n",
      "Read lmdb\n",
      "Loading training split: 9\n",
      "Read lmdb\n",
      "Patches for training: 7691\n",
      "\n",
      "Loading validation split: 0\n",
      "Read lmdb\n",
      "Patches for validation: 994\n",
      "\n",
      "Loading test split: 1\n",
      "Read lmdb\n",
      "Patches for test: 1004\n",
      "\n",
      "Number of free parameters:  66051\n",
      "10ep0\n",
      "\n",
      "EPOCH:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932cb2804612405e9eca83fc0046e69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a122958a1874e0ca75397f5e137dd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.6769677996635437  Train loss:  0.4159485101699829 \n",
      "\n",
      "Val acc:  0.44768611670020103  Train acc:  0.7174619685346523 \n",
      "\n",
      "Val AUC:  0.0 Train AUC:  0.0\n",
      "Val f1:  0.44768611670020103  Train f1:  0.7174619685346523 \n",
      "\n",
      "Val CONF: \n",
      " [[ 82.   0.   0.]\n",
      " [306. 333.  52.]\n",
      " [ 69. 122.  30.]] \n",
      "Train CONF: \n",
      " [[3351.  433.   25.]\n",
      " [ 637. 2108.  113.]\n",
      " [ 232.  733.   59.]] \n",
      "\n",
      "Learning Rate:  0.001\n",
      "\n",
      "EPOCH:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5020c7979fb6430bb2346c4ab393a77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800e80e82b7f43129bacb762014bf73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.4576139748096466  Train loss:  0.36717677116394043 \n",
      "\n",
      "Val acc:  0.694164989939638  Train acc:  0.7497074502665464 \n",
      "\n",
      "Val AUC:  0.0 Train AUC:  0.0\n",
      "Val f1:  0.694164989939638  Train f1:  0.7497074502665464 \n",
      "\n",
      "Val CONF: \n",
      " [[ 76.   6.   0.]\n",
      " [ 77. 614.   0.]\n",
      " [ 14. 207.   0.]] \n",
      "Train CONF: \n",
      " [[3494.  371.   13.]\n",
      " [ 531. 2238.   35.]\n",
      " [ 158.  817.   34.]] \n",
      "\n",
      "Learning Rate:  0.001\n",
      "\n",
      "EPOCH:  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7f974cd454d868f0bcae8dc572aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79940ca017c743baa9259f4a45a86bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5068979263305664  Train loss:  0.34317702054977417 \n",
      "\n",
      "Val acc:  0.6509054325955737  Train acc:  0.7725913405278909 \n",
      "\n",
      "Val AUC:  0.0 Train AUC:  0.0\n",
      "Val f1:  0.6509054325955737  Train f1:  0.7725913405278909 \n",
      "\n",
      "Val CONF: \n",
      " [[ 78.   4.   0.]\n",
      " [124. 563.   4.]\n",
      " [ 31. 184.   6.]] \n",
      "Train CONF: \n",
      " [[3486.  289.    6.]\n",
      " [ 416. 2443.   18.]\n",
      " [ 152.  868.   13.]] \n",
      "\n",
      "Learning Rate:  0.001\n",
      "\n",
      "EPOCH:  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095e376e8aee4beaab6ae7811a70d069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003e9fcdd49840878459606ffec28562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.5053916573524475  Train loss:  0.3222598135471344 \n",
      "\n",
      "Val acc:  0.682092555331992  Train acc:  0.7881939929788082 \n",
      "\n",
      "Val AUC:  0.0 Train AUC:  0.0\n",
      "Val f1:  0.682092555331992  Train f1:  0.7881939929788082 \n",
      "\n",
      "Val CONF: \n",
      " [[ 78.   4.   0.]\n",
      " [ 91. 600.   0.]\n",
      " [ 18. 203.   0.]] \n",
      "Train CONF: \n",
      " [[3542.  262.    6.]\n",
      " [ 370. 2496.   28.]\n",
      " [ 129.  834.   24.]] \n",
      "\n",
      "Learning Rate:  0.001\n",
      "\n",
      "EPOCH:  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761c3474e5b84448beb2bc95a0d21116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8cce3ab58c467b963ab6a2d118fe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.43061307072639465  Train loss:  0.31929072737693787 \n",
      "\n",
      "Val acc:  0.7364185110663984  Train acc:  0.785983617214928 \n",
      "\n",
      "Val AUC:  0.0 Train AUC:  0.0\n",
      "Val f1:  0.7364185110663984  Train f1:  0.785983617214928 \n",
      "\n",
      "Val CONF: \n",
      " [[ 65.  17.   0.]\n",
      " [ 24. 667.   0.]\n",
      " [  7. 214.   0.]] \n",
      "Train CONF: \n",
      " [[3457.  257.    6.]\n",
      " [ 342. 2555.   30.]\n",
      " [ 114.  897.   33.]] \n",
      "\n",
      "Learning Rate:  0.001\n",
      "\n",
      "EPOCH:  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b97232ee75f401fb9408df68a2191f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01e7797c5a04a4c935e0b0e8c04f736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.47104737162590027  Train loss:  0.29598432779312134 \n",
      "\n",
      "Val acc:  0.7012072434607642  Train acc:  0.8096476400988175 \n",
      "\n",
      "Val AUC:  0.0 Train AUC:  0.0\n",
      "Val f1:  0.7012072434607642  Train f1:  0.8096476400988175 \n",
      "\n",
      "Val CONF: \n",
      " [[ 74.   8.   0.]\n",
      " [ 68. 623.   0.]\n",
      " [ 11. 210.   0.]] \n",
      "Train CONF: \n",
      " [[3.654e+03 1.710e+02 3.000e+00]\n",
      " [2.920e+02 2.542e+03 3.400e+01]\n",
      " [1.110e+02 8.530e+02 3.100e+01]] \n",
      "\n",
      "Learning Rate:  0.001\n",
      "\n",
      "EPOCH:  7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2851c1139e14965acc5e27bef2abf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choosing only the images from the according splits (10CV)\n",
    "\n",
    "SPLITS = 1 # Number of iterations > n_splits\n",
    "n_splits = 10 # Number of splits to use\n",
    "\n",
    "for SPLIT in range(SPLITS):\n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    TRAIN_SPLITS = list(range(n_splits))\n",
    "    # We take out the SPLIT and SPLIT+1 sets for val and testing\n",
    "    if SPLIT == n_splits: # For the final split for validation we take the firs one for test\n",
    "        TRAIN_SPLITS.remove(0) \n",
    "    else:\n",
    "        TRAIN_SPLITS.remove(SPLIT+1)\n",
    "    TRAIN_SPLITS.remove(SPLIT)\n",
    "\n",
    "    train_set = dataset.PatchDataset([], [], [])\n",
    "    val_set = dataset.PatchDataset([], [], [])\n",
    "    test_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "    # Loading training splits:\n",
    "    for i in TRAIN_SPLITS:\n",
    "        print(f\"Loading training split: {i}\")\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{i}\"\n",
    "        \n",
    "        X_, y_, _ , _ = data_reader.read_lmdb(f\"D:/data/WSI/COAD/patches/{SPLIT_NAME}\")\n",
    "\n",
    "        train_set.inputs.extend(X_)\n",
    "        train_set.labels.extend(y_)\n",
    "\n",
    "    # Random oversampler\n",
    "\n",
    "    y =  [i[0] for i in train_set.labels]\n",
    "    positive = sum(y)\n",
    "    class_sample_count = np.array([len(y)-positive ,positive])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in y])\n",
    "    samples_weight = torch.from_numpy(samples_weight) # Probability for a sample to be sampled\n",
    "    #samples_weight = torch.tensor([1/len(samples_weight)]*len(samples_weight))\n",
    "    sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))#, replacement=False)\n",
    "    # Will take len(samples_weight) number of samples, this can be changed\n",
    "    \n",
    "    # Creating DataLoader\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "    \n",
    "    print(f\"Patches for training: {len(train_set)}\\n\")\n",
    "    \n",
    "    # Loading validation splits:\n",
    "    SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    print(f\"Loading validation split: {SPLIT}\")\n",
    "\n",
    "    X_, y_, _ , _ = data_reader.read_lmdb(f\"D:/data/WSI/COAD/patches/{SPLIT_NAME}\")\n",
    "\n",
    "    val_set.inputs.extend(X_)\n",
    "    val_set.labels.extend(y_)\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Patches for validation: {len(val_set)}\\n\")\n",
    "\n",
    "    # Loading validation splits:\n",
    "    if SPLIT == n_splits:\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{0}\"\n",
    "        print(f\"Loading test split: {0}\")\n",
    "    else:\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT+1}\"\n",
    "        print(f\"Loading test split: {SPLIT+1}\")\n",
    "\n",
    "    X_, y_, _ , _ = data_reader.read_lmdb(f\"D:/data/WSI/COAD/patches/{SPLIT_NAME}\")\n",
    "\n",
    "    test_set.inputs.extend(X_)\n",
    "    test_set.labels.extend(y_)\n",
    "\n",
    "    test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Patches for test: {len(test_set)}\\n\")\n",
    "\n",
    "    init()\n",
    "\n",
    "    train()\n",
    "\n",
    "    del train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(NAME):\n",
    "  fig, axs = plt.subplots(2, figsize = (7,6))\n",
    "\n",
    "  acc_df = pd.read_csv(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\log\\\\model_{NAME}.log\")\n",
    "  acc_df.columns = [\"MODEL_NAME\", \"TIME\", \"ACC\", \"LOSS\", \"CONF_M\", \"AUC\",\n",
    "                     \"VAL_ACC\", \"VAL_LOSS\", \"VAL_CONF_M\", \"VAL_AUC\"]\n",
    "\n",
    "  fig, axs = plt.subplots(2, figsize=(5,7))\n",
    "\n",
    "  axs[0].legend(\"MODEL_NAME\", loc=2)\n",
    "\n",
    "  acc_df.plot(y=\"ACC\", ax=axs[0])\n",
    "  acc_df.plot(y=\"VAL_ACC\", ax=axs[0])\n",
    "\n",
    "  acc_df.plot(y=\"LOSS\", ax=axs[1])\n",
    "  acc_df.plot(y=\"VAL_LOSS\",ax=axs[1])\n",
    "\n",
    "  fig.show()\n",
    "  fig.savefig(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\lc\\\\l_curve_{NAME}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXP_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\src\\WSI\\wsi_train.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/src/WSI/wsi_train.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m SPLIT \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/src/WSI/wsi_train.ipynb#ch0000004?line=1'>2</a>\u001b[0m     NAME \u001b[39m=\u001b[39m EXP_NAME \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mSPLIT\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/src/WSI/wsi_train.ipynb#ch0000004?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(NAME)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/src/WSI/wsi_train.ipynb#ch0000004?line=3'>4</a>\u001b[0m     learning_curve(NAME)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EXP_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "for SPLIT in range(1):\n",
    "    NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "    print(NAME)\n",
    "    learning_curve(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de74446a7cd3a56c425ecbffb2a7ad915342ddab3c48353acb4566475bd7705f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openslide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
