{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import time\n",
    "import torchstain\n",
    "\n",
    "import data_reader\n",
    "import dataset\n",
    "from plots import learning_curve_train\n",
    "\n",
    "from normalization import normalize_staining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU : cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Choosing device for tensor processing\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU :\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and data variables\n",
    "\n",
    "MODE = \"w\"\n",
    "EXP_NAME = \"256_tcia_mac\" # \"test_normalization\"#\"256_tcia_25ep\"#\"tcia_effnet\"\n",
    "\n",
    "# DATA_SET_NAME = \"data_set_X20_100%_SPLIT\" # f\"tcia_data_set_SPLIT\" # f\"train_256_tcia_data_set_SPLIT\" #\n",
    "# DATA_SET_NAME = \"train_512_gdc+tcia_normal_data_set_SPLIT\"\n",
    "DATA_SET_NAME = \"train_256_tcia_mac\"\n",
    "PATCH_SIZE = 512\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "\n",
    "SIZE_X = PATCH_SIZE\n",
    "SIZE_Y = PATCH_SIZE\n",
    "\n",
    "fine_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m): # XAVIER initialization for final layer weight initialization\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def init(fine_tuning=False):\n",
    "    global net, loss_function, scheduler, optimizer, MODEL_NAME\n",
    "\n",
    "    if fine_tuning == True:\n",
    "        print(f\"Loading {MODEL_NAME}\")\n",
    "        net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "\n",
    "        for param in net.layer4[1].parameters():\n",
    "           param.requires_grad = True\n",
    "\n",
    "        '''\n",
    "        for param in net.features[8].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in net.features[9].parameters():# net.layer4[1].parameters():\n",
    "            param.requires_grad = True # Unfreezing the last residual block\n",
    "        '''\n",
    "\n",
    "        n_params = sum(p.numel() for p in net.fc.parameters())# + sum(p.numel() for p in net.features[8].parameters())\n",
    "\n",
    "        learning_rate = 1E-5#1E-8 # LR\n",
    "\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08)# Optimizer\n",
    "\n",
    "    else:\n",
    "        net = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "        #net = torchvision.models.efficientnet_b1(pretrained=True).to(device) \n",
    "    \n",
    "        net.fc = nn.Sequential(\n",
    "                    nn.Dropout(p=0.5),  \n",
    "                    nn.Linear(512, 2),\n",
    "                    #nn.ReLU(inplace=True),\n",
    "                    #nn.Linear(512, 2),\n",
    "                    nn.Softmax(dim = -1)\n",
    "                    ).to(device)\n",
    "\n",
    "        for param in net.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        net.fc.apply(init_weights) # Xavier init\n",
    "\n",
    "        n_params = sum(p.numel() for p in net.fc.parameters())# + sum(p.numel() for p in net.layer4[1].parameters())\n",
    "    \n",
    "        learning_rate = 1E-4\n",
    "\n",
    "        optimizer = optim.Adam(net.fc.parameters(), lr=learning_rate)#, betas=(0.9, 0.999), eps=1e-08)# Optimizer\n",
    "\n",
    "    print(\"Number of free parameters: \", n_params)\n",
    "    #Hyperparameters:\n",
    "    loss_function = nn.CrossEntropyLoss()  # Loss\n",
    "    lambda1 = lambda epoch: 0.8 ** epoch # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, case_ids, train=False):\n",
    "    if train: \n",
    "        net.zero_grad()\n",
    "\n",
    "    \"\"\"# TOTAL MEAN AND STD DEV\n",
    "    mean_set = [166.44544832, 115.25740225, 149.71458135]\n",
    "    std_set = [51.04418855, 53.39864174, 43.56392919]\n",
    "    normalize_set = torchvision.transforms.Normalize(mean=mean_set, std=std_set)\n",
    "    # NORMALIZATION\n",
    "    mean_gtex = [144.84250856,  90.71206166, 128.46787316]\n",
    "    std_gtex = [61.96567854, 60.13392162, 51.13692362]\n",
    "    normalize_gtex = torchvision.transforms.Normalize(mean=mean_gtex, std=std_gtex)\n",
    "\n",
    "    mean_tcga = [190.30330768, 142.36479088, 173.17902561]\n",
    "    std_tcga = [38.98271104, 45.96033188, 35.20046394]\n",
    "    normalize_tcga = torchvision.transforms.Normalize(mean=mean_tcga, std=std_tcga)\"\"\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    normalize = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "\n",
    "    for i, (case_id, x) in enumerate(zip(case_ids, X)):\n",
    "        X[i] = normalize(X[i]/255)\n",
    "    \n",
    "    #plt.imshow(X[0].permute(2,1,0).to(\"cpu\"))\n",
    "    #plt.show()\n",
    "    outputs = net(X)\n",
    "    \n",
    "    y_pred = [torch.argmax(i) for i in outputs.cpu()]\n",
    "    y_true = [torch.argmax(i) for i in y.cpu()]\n",
    "\n",
    "    loss = loss_function(outputs, y)\n",
    "    conf_m = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
    "    auc = 0# sklearn.metrics.roc_auc_score(y_true, y_pred, labels=[0, 1])# average= \"micro\" multi_class=\"ovr\") #!!!!\n",
    "\n",
    "    if train:\n",
    "        loss.backward() # Calculate gradients using backprop\n",
    "        #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.0001)# To prevent exploding or vanishing gradients\n",
    "        optimizer.step() # Updates W and b using previously calculated gradients\n",
    "\n",
    "    return [acc, loss, conf_m, f1, auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  global net, loss_function, scheduler, optimizer, train_set, val_set, MODEL_NAME, EPOCHS, val_dataloader, train_dataloader\n",
    "  \n",
    "  print(MODEL_NAME)\n",
    "  patience, prev_val_loss = 0, 0\n",
    "\n",
    "  with open(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\log\\\\model_{MODEL_NAME}.log\", MODE) as f:\n",
    "    for epoch in range(EPOCHS):\n",
    "      acc, loss, f1, auc = 0, 0, 0, 0\n",
    "      val_acc, val_loss, val_f1, val_auc = 0, 0, 0, 0\n",
    "      conf_m, val_conf_m = np.zeros((2,2)), np.zeros((2,2))\n",
    "\n",
    "      print(\"\\nEPOCH: \", epoch+1)\n",
    "\n",
    "      n_patches = sum(len(batch[0]) for batch in iter(train_dataloader))\n",
    "\n",
    "      for batch_X, batch_y, patch_ids in tqdm(iter(train_dataloader)):\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0,3,2,1), batch_y.type(torch.FloatTensor).to(device) \n",
    "        \n",
    "        net.train() # Making sure that the model is in training mode\n",
    "        \n",
    "        performance_metrics = fwd_pass(batch_X, batch_y, patch_ids, train=True)\n",
    "        acc_aux, loss_aux, conf_m_aux, f1_aux, auc_aux = performance_metrics[0], performance_metrics[1], performance_metrics[2], performance_metrics[3], performance_metrics[4]\n",
    "        acc += acc_aux*(len(batch_X)/n_patches) # Calculating the average loss and acc through batches sum ACCi*Wi/N (Wi = weight of the batch)\n",
    "        loss += loss_aux*(len(batch_X)/n_patches)\n",
    "        conf_m += conf_m_aux\n",
    "        f1 += f1_aux*(len(batch_X)/n_patches)\n",
    "        auc += auc_aux*(len(batch_X)/n_patches)\n",
    "\n",
    "      for batch_X, batch_y, patch_ids in tqdm(iter(val_dataloader)):\n",
    "\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1), batch_y.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        net.eval() # Making sure that the model is not training and deactivate droptout\n",
    "        \n",
    "        with torch.no_grad(): # Disable all computations, works together with net.eval()\n",
    "          performance_metrics = fwd_pass(batch_X, batch_y, patch_ids, train=False)\n",
    "          \n",
    "        acc_aux, loss_aux, conf_m_aux, f1_aux, auc_aux = performance_metrics[0], performance_metrics[1], performance_metrics[2], performance_metrics[3], performance_metrics[4]  \n",
    "        val_acc += acc_aux*(len(batch_X)/len(val_set)) # Calculating the average loss and acc through batches sum ACCi*Wi/N (Wi = weight of the batch)\n",
    "        val_loss += loss_aux*(len(batch_X)/len(val_set))\n",
    "        val_conf_m += conf_m_aux\n",
    "        val_f1 += f1_aux*(len(batch_X)/len(val_set))\n",
    "        val_auc += auc_aux*(len(batch_X)/len(val_set))\n",
    "\n",
    "        \n",
    "      print(\"Val loss: \", val_loss.item(), \" Train loss: \", loss.item(), \"\\n\")\n",
    "      print(\"Val acc: \", val_acc, \" Train acc: \", acc, \"\\n\")\n",
    "      print(\"Val AUC: \", val_auc, \" Train AUC: \", auc,\"\\n\")\n",
    "      print(\"Val f1: \", val_f1, \" Train f1: \", f1, \"\\n\")\n",
    "      print(\"Val CONF: \\n\", val_conf_m, \"\\nTrain CONF: \\n\", conf_m, \"\\n\")\n",
    "\n",
    "      conf_m = f\"{conf_m[0][0]}+{conf_m[0][1]}+{conf_m[1][0]}+{conf_m[1][1]}\"\n",
    "      val_conf_m = f\"{val_conf_m[0][0]}+{val_conf_m[0][1]}+{val_conf_m[1][0]}+{val_conf_m[1][1]}\"\n",
    "    \n",
    "      f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),3)},{round(float(loss),4)},{conf_m},{round(float(auc),4)},\")\n",
    "      f.write(f\"{round(float(val_acc),3)},{round(float(val_loss),4)},{val_conf_m}, {round(float(val_auc),4)}\\n\")\n",
    "      f.write(\"\\n\\n\")\n",
    "\n",
    "      # Early stopping, if the difference between loss and validation loss \n",
    "      # is bigger than the threshold for 3 epochs in a row training is stopped\n",
    "      if val_loss.item()>prev_val_loss:\n",
    "        patience +=1\n",
    "      else:\n",
    "        patience = 0\n",
    "\n",
    "      print(\"Learning Rate: \", optimizer.param_groups[0][\"lr\"])\n",
    "      scheduler.step() # Changing the learning rate\n",
    "\n",
    "      if patience >= 2:\n",
    "        print(\"Stopping early: \")\n",
    "        break\n",
    "      prev_val_loss = val_loss.item()\n",
    "\n",
    "      torch.save(net, f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training split: 0\n",
      "Read lmdb\n",
      "474\n",
      "Patches for training: 474\n",
      "\n",
      "Loading validation split: 1\n",
      "Read lmdb\n",
      "481\n",
      "Patches for validation: 481\n",
      "\n",
      "Number of free parameters:  1026\n",
      "256_tcia_mac1\n",
      "\n",
      "EPOCH:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:146: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45bcdee68274464a009d8c02ac02eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce01bb7c8e3f4246980059e42d74b27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7582059502601624  Train loss:  0.7370514273643494 \n",
      "\n",
      "Val acc:  0.41372141372141374  Train acc:  0.5144927536231884 \n",
      "\n",
      "Val AUC:  0.0  Train AUC:  0.0 \n",
      "\n",
      "Val f1:  0.400586384344422  Train f1:  0.47322209107106133 \n",
      "\n",
      "Val CONF: \n",
      " [[ 98.  40.]\n",
      " [242. 101.]] \n",
      "Train CONF: \n",
      " [[97. 53.]\n",
      " [81. 45.]] \n",
      "\n",
      "Learning Rate:  0.0001\n",
      "\n",
      "EPOCH:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac19cfb098bb4863a2e8a4e5725cc5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de985a87c080400c982ad975133c46e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  0.7891261577606201  Train loss:  0.7232394814491272 \n",
      "\n",
      "Val acc:  0.3762993762993763  Train acc:  0.5181159420289855 \n",
      "\n",
      "Val AUC:  0.0  Train AUC:  0.0 \n",
      "\n",
      "Val f1:  0.36198800523639774  Train f1:  0.4616933255855926 \n",
      "\n",
      "Val CONF: \n",
      " [[112.  26.]\n",
      " [274.  69.]] \n",
      "Train CONF: \n",
      " [[109.  36.]\n",
      " [ 97.  34.]] \n",
      "\n",
      "Learning Rate:  8e-05\n",
      "Stopping early: \n"
     ]
    }
   ],
   "source": [
    "# Choosing only the images from the according splits (10CV)\n",
    "\n",
    "SPLITS = [1] # Number of iterations > n_splits\n",
    "n_splits = 3 # Number of splits to use\n",
    "\n",
    "for SPLIT in SPLITS:\n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    TRAIN_SPLITS = list(range(n_splits))\n",
    "    # We take out the SPLIT and SPLIT+1 sets for val and testing\n",
    "    if SPLIT == n_splits-1: # For the final split for validation we take the first one for test\n",
    "        TRAIN_SPLITS.remove(0) \n",
    "    else:\n",
    "        TRAIN_SPLITS.remove(SPLIT+1)\n",
    "    TRAIN_SPLITS.remove(SPLIT)\n",
    "\n",
    "    train_set = dataset.PatchDataset([], [], [])\n",
    "    val_set = dataset.PatchDataset([], [], [])\n",
    "    test_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "    # Loading training splits:\n",
    "    for i in TRAIN_SPLITS:\n",
    "        print(f\"Loading training split: {i}\")\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{i}\"\n",
    "        \n",
    "        X_, y_, _ , patch_ids_ = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "        train_set.inputs.extend(X_)\n",
    "        train_set.labels.extend(y_)\n",
    "        train_set.case_ids.extend(patch_ids_)\n",
    "\n",
    "    # Random oversampler\n",
    "    negative =  [i[0] for i in train_set.labels]\n",
    "    positive =  [i[1] for i in train_set.labels]\n",
    "\n",
    "    class_sample_count = np.array([sum(negative), sum(positive)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[np.argmax(t)] for t in train_set.labels])\n",
    "    samples_weight = torch.from_numpy(samples_weight) # Probability for a sample to be sampled\n",
    "    #samples_weight = torch.tensor([1/len(samples_weight)]*len(samples_weight))\n",
    "    sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), 2*int(sum(negative)))#, replacement=False) #int(len(train_set)))#\n",
    "    # Will take len(samples_weight) number of samples, this can be changed\n",
    "\n",
    "    # Creating DataLoader\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "    \n",
    "    print(f\"Patches for training: {len(train_set)}\\n\")\n",
    "    \n",
    "    # Loading validation splits:\n",
    "    SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    print(f\"Loading validation split: {SPLIT}\")\n",
    "\n",
    "    X_, y_, _ , patch_ids_ = data_reader.read_lmdb(f\"D:/data/WSI/patches/{SPLIT_NAME}\")\n",
    "\n",
    "    val_set.inputs.extend(X_)\n",
    "    val_set.labels.extend(y_)\n",
    "    val_set.case_ids.extend(patch_ids_)\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Patches for validation: {len(val_set)}\\n\")\n",
    "\n",
    "    init(fine_tuning=fine_tuning)\n",
    "\n",
    "    train()\n",
    "\n",
    "    del train_set, val_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for SPLIT in range(1):\n",
    "    NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "    print(NAME)\n",
    "    learning_curve_train(NAME)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de74446a7cd3a56c425ecbffb2a7ad915342ddab3c48353acb4566475bd7705f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openslide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
