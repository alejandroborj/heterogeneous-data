{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "import dataset\n",
    "import data_reader\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU : cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Choosing device for tensor processing\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU :\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"test_normalization\"#\"256_tcia_25ep\"#\"25ep\"#\"tcia_25ep\"#\n",
    "\n",
    "DATA_SET_NAME = f\"data_set_X20_100%_SPLIT\"#\"tcia_data_set_SPLIT\"#\"train_256_tcia_data_set_SPLIT\"#\"mhmc\"#\"test_tcia_data_set_SPLIT\"#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsi_class(wsi_data_set):\n",
    "    global net\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - wsi_data_set, patches from a given WSI, dataset object\n",
    "    - y, patch labels\n",
    "    Outputs:\n",
    "    - output, Prediction\n",
    "    \"\"\"\n",
    "    mean_gtex = [144.84250856,  90.71206166, 128.46787316]\n",
    "    std_gtex = [61.96567854, 60.13392162, 51.13692362]\n",
    "    normalize_gtex = transforms.Normalize(mean=mean_gtex, std=std_gtex)\n",
    "\n",
    "    mean_tcga = [190.30330768, 142.36479088, 173.17902561]\n",
    "    std_tcga = [38.98271104, 45.96033188, 35.20046394] \n",
    "    normalize_tcga = transforms.Normalize(mean=mean_tcga, std=std_tcga)\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    wsi_dataloader = DataLoader(wsi_data_set, batch_size=64)\n",
    "    outputs = []\n",
    "    loss = 0\n",
    "\n",
    "    for batch_X, batch_y, case_ids in iter(wsi_dataloader):\n",
    "        batch_X, batch_y = batch_X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1), batch_y.type(torch.FloatTensor).to(device) \n",
    "\n",
    "        # NORMALIZATION\n",
    "        for i, (case_id, x) in enumerate(zip(case_ids, batch_X)):\n",
    "            if \"TCGA\" in case_id:\n",
    "                batch_X[i] = normalize_tcga(batch_X[i])\n",
    "                batch_X[i] = normalize(batch_X[i])\n",
    "            elif \"GTEX\" in case_id:\n",
    "                batch_X[i] = normalize_gtex(batch_X[i])\n",
    "                batch_X[i] = normalize(batch_X[i])\n",
    "            else:\n",
    "                batch_X[i] = normalize(batch_X[i]/255)\n",
    "    \n",
    "        output = net(batch_X)\n",
    "        outputs.extend(output.cpu())\n",
    "        \n",
    "        loss += nn.CrossEntropyLoss()(output, batch_y).cpu()\n",
    "\n",
    "    #print(outputs)\n",
    "\n",
    "    y_pred = [torch.argmax(i) for i in outputs] # 1 means positive diagnosis: (1,0) => 1\n",
    "    prob_neg = y_pred.count(0)/len(y_pred)\n",
    "    prob_pos = y_pred.count(1)/len(y_pred)\n",
    "\n",
    "    output = [prob_neg, prob_pos]\n",
    "    loss = loss/len(wsi_data_set)\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_set_X20_100%_SPLIT1\n",
      "Read lmdb\n",
      "8207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4dc901bb0445eb83c6f3a105596311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:146: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss:  0.012045246\n",
      "Acc:  0.6538461538461539\n",
      "Bacc:  0.5\n",
      "f1:  0.39534883720930236\n",
      "CONF: \n",
      " [[34  0]\n",
      " [18  0]] \n",
      "\n",
      "data_set_X20_100%_SPLIT2\n",
      "Read lmdb\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "tot_conf = np.zeros((2,2))\n",
    "tot_out, tot_labels, accs, f1s, losses, baccs = [], [], [], [], [], []\n",
    "\n",
    "for SPLIT in range(n_splits):\n",
    "\n",
    "    test_out = pd.DataFrame(columns=[\"Sample ID\", \"Output\", \"Label\"])\n",
    "    \n",
    "    if SPLIT == n_splits-1: # For the final split for validation we take the firs one for test\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{0}\" # Test set \n",
    "    else:\n",
    "        SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT+1}\" # Test set\n",
    "    \n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "    \n",
    "    MODEL_NAME = \"test_normalization1\"#\"train_512_gtex+gdc1\"\n",
    "\n",
    "    print(SPLIT_NAME)\n",
    "    net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "\n",
    "    if \"mhmc\" in DATA_SET_NAME:    \n",
    "        split = open(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\splits\\\\mhmc_testsplit{SPLIT}.txt\", \"r\").read().split()\n",
    "        path = \"D:/data/WSI/MHMC/TMA\"\n",
    "        X, y, case_ids = data_reader.read_mhmc(path, split)\n",
    "        \n",
    "        sample_ids = [\"_\".join(case_id.split(\"_\")[:-2]) for case_id in case_ids] # Taking only the sample_id, not patch id\n",
    "\n",
    "    else:\n",
    "        X, y, _ , case_ids = data_reader.read_lmdb(f\"D:\\data\\WSI\\patches\\{SPLIT_NAME}\")\n",
    "\n",
    "        sample_ids = [case_id.split(\"_\")[1] for case_id in case_ids] # Taking only the sample_id, not patch id\n",
    "\n",
    "    outputs, loss, labels = [], [], []\n",
    "    unique_sample_ids = np.unique(sample_ids)\n",
    "    sample_ids = np.array(sample_ids)\n",
    "\n",
    "    for unique_sample_id in tqdm(unique_sample_ids):\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            ii = np.where(sample_ids == unique_sample_id)[0]\n",
    "\n",
    "            wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "            wsi_data_set.inputs.extend(X[ii[0]:ii[-1]])# Taking the patches from a given wsi\n",
    "            wsi_data_set.labels.extend(y[ii[0]:ii[-1]])\n",
    "            wsi_data_set.case_ids.extend(sample_ids)\n",
    "\n",
    "            if len(wsi_data_set) == 0:\n",
    "                print(\"No patches\")\n",
    "                outputs.append([np.nan,np.nan])\n",
    "                labels.append(np.argmax(y[ii[0]]))\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                output = wsi_class(wsi_data_set)\n",
    "                outputs.append(output[0])\n",
    "                loss.append(output[1])\n",
    "                labels.append(np.argmax(y[ii[0]])) # OHE to binary\n",
    "\n",
    "            del wsi_data_set\n",
    "\n",
    "    loss = np.mean(loss)\n",
    "\n",
    "    out = [np.argmax(output) for output in outputs]\n",
    "\n",
    "    conf_m = sklearn.metrics.confusion_matrix(labels, out, labels=[0, 1])\n",
    "    acc = sklearn.metrics.accuracy_score(labels, out)\n",
    "    bacc = sklearn.metrics.balanced_accuracy_score(labels, out)\n",
    "    f1 = sklearn.metrics.f1_score(labels, out,average=\"macro\")\n",
    "    auc = 0 #sklearn.metrics.roc_auc_score(y_true, y_pred, average= \"micro\", multi_class=\"ovr\") #!!!!\n",
    "\n",
    "    tot_out += out\n",
    "    tot_labels += labels\n",
    "\n",
    "    print(\"Mean Loss: \", loss)\n",
    "    print(\"Acc: \", acc)\n",
    "    print(\"Bacc: \", bacc)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"CONF: \\n\", conf_m, \"\\n\")\n",
    "\n",
    "    tot_conf += conf_m\n",
    "\n",
    "    plots.plot_conf(SPLIT_NAME, conf_m)\n",
    "    \n",
    "    '''\n",
    "    test_out[\"Sample ID\"] = unique_sample_ids # The samples are ordered since the loop goes through unique_sample_ids\n",
    "    test_out[\"Output\"] = [output[0] for output in outputs]\n",
    "    test_out[\"Label\"] = labels\n",
    "\n",
    "    #test_out.to_csv(f\"D:\\data\\Outputs\\WSI\\PDAC_test_{SPLIT+1}.csv\")'''\n",
    "\n",
    "    accs.append(acc)\n",
    "    baccs.append(bacc)\n",
    "    f1s.append(f1)\n",
    "    losses.append(loss)\n",
    "\n",
    "tot_acc = np.mean(accs)\n",
    "tot_bacc = np.mean(baccs)\n",
    "tot_f1 = np.mean(f1s)\n",
    "tot_loss = np.mean(losses)\n",
    "\n",
    "err_acc = np.std(accs)\n",
    "err_bacc = np.std(baccs)\n",
    "err_f1 = np.std(f1s)\n",
    "err_loss = np.std(losses)\n",
    "\n",
    "print(tot_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACC:\", round(tot_acc,3),\" +/- \", round(err_acc,3))\n",
    "print(\"BACC:\", round(tot_bacc,3),\" +/- \", round(err_bacc,3))\n",
    "print(\"F1:\", round(tot_f1,3),\" +/- \", round(err_f1,3))\n",
    "\n",
    "plots.plot_conf(\"10CV\", tot_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = []\n",
    "\n",
    "# ROC\n",
    "for i in reversed(np.linspace(0,1,10000)):\n",
    "    out = [1 if output>i else 0 for output in outputs]\n",
    "    conf_m = confusion_matrix(labels, out, labels=[0, 1])\n",
    "    roc.append(conf_m[1][1]/(conf_m[1][0]+conf_m[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_roc(SPLIT_NAME, roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "tot_conf = np.zeros((2,2))\n",
    "tot_out, tot_labels = [], []\n",
    "\n",
    "#Generating the training outputs PDAC_TEST_(MODEL NUMBER)_(SPLIT OUTPUT)\n",
    "\n",
    "for TEST_SPLIT in [0,1,2,3]:#range(n_splits):\n",
    "\n",
    "    MODEL_NAME = EXP_NAME + f\"{TEST_SPLIT}\"\n",
    "    net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "\n",
    "    for SPLIT in range(n_splits):\n",
    "        #This has already been executed\n",
    "        if SPLIT+1==TEST_SPLIT:\n",
    "            pass\n",
    "\n",
    "        test_out = pd.DataFrame(columns=[\"Sample ID\", \"Output\", \"Label\"])\n",
    "        \n",
    "        if SPLIT == n_splits-1: # For the final split for validation we take the first one for test\n",
    "            SPLIT_NAME = DATA_SET_NAME + f\"{0}\" # Test set \n",
    "        else:\n",
    "            SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT+1}\" # Test set\n",
    "\n",
    "        print(SPLIT_NAME)\n",
    "\n",
    "        X, y, _ , case_ids = data_reader.read_lmdb(f\"D:\\data\\WSI\\patches\\{SPLIT_NAME}\")\n",
    "\n",
    "        sample_ids = [case_id.split(\"_\")[1] for case_id in case_ids] # Taking only the sample_id, not patch id\n",
    "\n",
    "        outputs, loss, labels = [], [], []\n",
    "        unique_sample_ids = np.unique(sample_ids)\n",
    "        sample_ids = np.array(sample_ids)\n",
    "\n",
    "        for unique_sample_id in tqdm(unique_sample_ids):\n",
    "\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "        \n",
    "                ii = np.where(sample_ids == unique_sample_id)[0]\n",
    "\n",
    "                wsi_data_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "                wsi_data_set.inputs.extend(X[ii[0]:ii[-1]])# Taking the patches from a given wsi\n",
    "                wsi_data_set.labels.extend(y[ii[0]:ii[-1]])\n",
    "                wsi_data_set.case_ids.extend(sample_ids)\n",
    "\n",
    "                if len(wsi_data_set) == 0:\n",
    "                    print(\"No patches\")\n",
    "                    outputs.append([np.nan,np.nan])\n",
    "                    labels.append(np.argmax(y[ii[0]]))\n",
    "                    pass\n",
    "                else:\n",
    "                    output = wsi_class(wsi_data_set)\n",
    "                    outputs.append(output[0])\n",
    "                    loss.append(output[1])\n",
    "                    labels.append(np.argmax(y[ii[0]])) # OHE to binary\n",
    "\n",
    "                del wsi_data_set\n",
    "        \n",
    "        test_out[\"Sample ID\"] = unique_sample_ids # The samples are ordered since the loop goes through unique_sample_ids\n",
    "        test_out[\"Output\"] = [output[0] for output in outputs]\n",
    "        test_out[\"Label\"] = labels\n",
    "\n",
    "        test_out.to_csv(f\"D:\\data\\Outputs\\WSI\\PDAC_train_{TEST_SPLIT}_{SPLIT+1}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de74446a7cd3a56c425ecbffb2a7ad915342ddab3c48353acb4566475bd7705f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openslide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
