{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import dataset\n",
    "import data_reader\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"ejemplo_2_\"\n",
    "\n",
    "DATA_SET_NAME = f\"data_set_x20_100%_split\"\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsi_class(X):\n",
    "    global net\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - X, patches from a given WSI.\n",
    "    - y, patch labels\n",
    "    Outputs:\n",
    "    - output, Prediction\n",
    "    \"\"\"\n",
    "    outputs = net(X)\n",
    "    y_pred = [torch.argmin(i) for i in outputs.cpu()] # 1 means positive diagnosis: (1,0) => 1\n",
    "    output = y_pred.sum()/len(y_pred)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read lmdb\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\pred_WSI.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000018?line=11'>12</a>\u001b[0m test_set\u001b[39m.\u001b[39mcase_ids\u001b[39m.\u001b[39mextend(case_ids_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000018?line=13'>14</a>\u001b[0m X \u001b[39m=\u001b[39m test_set\u001b[39m.\u001b[39minputs[:\u001b[39m16\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000018?line=15'>16</a>\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39mFloatTensor)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000018?line=17'>18</a>\u001b[0m wsi_class(X)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "SPLIT_NAME = DATA_SET_NAME + f\"{0}\"\n",
    "MODEL_NAME = EXP_NAME + f\"{0}\"\n",
    "\n",
    "net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "\n",
    "test_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "X_, y_, _ , case_ids_ = data_reader.read_lmdb(f\"C:/Users/Alejandro/Desktop/heterogeneous-data/data/patches/{SPLIT_NAME}\")\n",
    "\n",
    "test_set.inputs.extend(X_)\n",
    "test_set.labels.extend(y_)\n",
    "test_set.case_ids.extend(case_ids_)\n",
    "\n",
    "X = test_set.inputs[:16]\n",
    "\n",
    "X = X.type(torch.FloatTensor).to(device).permute(0, 3, 2, 1)\n",
    "\n",
    "wsi_class(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_wsi():\n",
    "    global net, test_set\n",
    "\n",
    "    # In order to obtain the correct order labels\n",
    "    \n",
    "    X = sorted(zip(X, ids_X))\n",
    "    y = sorted(zip(y, ids_y))\n",
    "\n",
    "    X, ids_X = zip(*X)\n",
    "    y, ids_y = zip(*y)\n",
    "    \n",
    "    test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Patches for test: {len(test_set)}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing only the images from the according splits (10CV)\n",
    "\n",
    "SPLITS = 1 # Number of iterations > n_splits\n",
    "\n",
    "for SPLIT in range(SPLITS):\n",
    "    MODEL_NAME = EXP_NAME + f\"{SPLIT}\"\n",
    "    SPLIT_NAME = DATA_SET_NAME + f\"{SPLIT}\"\n",
    "\n",
    "    test_set = dataset.PatchDataset([], [], [])\n",
    "\n",
    "    X_, y_, _ , case_ids_ = data_reader.read_lmdb(f\"C:/Users/Alejandro/Desktop/heterogeneous-data/data/patches/{SPLIT_NAME}\")\n",
    "\n",
    "    test_set.inputs.extend(X_)\n",
    "    test_set.labels.extend(y_)\n",
    "    test_set.case_ids.extend(case_ids_)\n",
    "\n",
    "    print(f\"Loading model {MODEL_NAME}\\n\")\n",
    "\n",
    "    net = torch.load(f\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\heterogeneous-data\\\\results\\\\WSI\\\\models\\\\{MODEL_NAME}.pth\") # Model loading\n",
    "\n",
    "    test_wsi()\n",
    "\n",
    "    del test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## In order to fix .dll openslide bug a path for said file is provided\n",
    "os.environ['PATH'] = r\"C:\\Users\\Alejandro\\Downloads\\openslide-win64-20171122\\openslide-win64-20171122\\bin\" + \";\" + os.environ['PATH']  #libvips-bin-path is where you save the libvips files\n",
    "import openslide\n",
    "import large_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_data(paths, patch_size, formats, net): #  paths => paths for all data folders. dataset => train, test or val\n",
    "    for path in tqdm(paths):\n",
    "        #case_id = os.path.split(path)[-1]\n",
    "        if not os.path.exists(path) or len(os.listdir(path)) == 0:\n",
    "            print(\"Path does not exist\")\n",
    "            pass\n",
    "        else:\n",
    "            file_data = []\n",
    "            for format in formats:\n",
    "                for file in glob.glob(path + r\"\\*\" + format):\n",
    "                    if file[-51:-49] in (\"01\", \"02\", \"03\", \"04\" ,\"05\", \"06\", \"07\", \"08\", \"09\"): # Reading the ID diagnostic sample type 01 == Primary tumor\n",
    "                        label = torch.tensor([1,0]).to(\"cuda:0\").float()\n",
    "                    else:\n",
    "                        label = torch.tensor([0,1]).to(\"cuda:0\").float()\n",
    "                    file_data += pred_file(file, patch_size, label, net)\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_file(file, patch_size, label, net):\n",
    "    \"\"\"\n",
    "    Classifies a WSI (file) using net classifier\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\"\n",
    "    ts = large_image.getTileSource(file)\n",
    "    \n",
    "    patches = []\n",
    "    labels = []\n",
    "    \n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    normalize = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "    \n",
    "    for tile_info in ts.tileIterator(\n",
    "    scale=dict(magnification=20),\n",
    "    tile_size=dict(width=patch_size, height=patch_size),\n",
    "    tile_overlap=dict(x=0, y=0),\n",
    "    format=large_image.tilesource.TILE_FORMAT_PIL\n",
    "    ):  \n",
    "        patch = tile_info['tile']\n",
    "\n",
    "        # Changing from PIL RGBA image to RGB tensor\n",
    "        patch_aux = Image.new(\"RGB\", patch.size, (255, 255, 255))\n",
    "        patch_aux.paste(patch, mask=patch.split()[3]) # 3 is the alpha channel\n",
    "\n",
    "        patch = np.asarray(patch_aux)\n",
    "\n",
    "        avg = patch.mean(axis=0).mean(axis=0)\n",
    "\n",
    "        if  avg[0]< 220 and avg[1]< 220 and avg[2]< 220 and patch.shape == (patch_size, patch_size, 3): # Checking if the patch is white and its a square tile\n",
    "            patch = (torch.from_numpy(patch)/255).to(device)\n",
    "            patch = normalize(patch.permute(2, 1, 0))\n",
    "            patches.append(patch)\n",
    "            labels.append(label)\n",
    "            \n",
    "    patches = torch.stack(patches).view(-1, 3, patch_size, patch_size)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    print(len(patches))\n",
    "    \n",
    "    acc, loss, output = fwd_pass(patches, labels, net)\n",
    "            \n",
    "    return acc, loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, net, train=False):\n",
    "    loss_function = loss_function = nn.BCELoss()\n",
    "\n",
    "    outputs = net(X)\n",
    "\n",
    "    matches = [torch.argmax(i) == torch.argmax(j) for i,j in zip(outputs, y)]\n",
    "\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "    \n",
    "    output = torch.sum(outputs, dim=0)\n",
    "    if output[0]>output[1]:\n",
    "        output = [1, 0]\n",
    "    else:\n",
    "        output = [0, 1]\n",
    "\n",
    "    del outputs, X, y\n",
    "\n",
    "    return acc, loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\data\\gdc_download_20220427_144600.480657\"\n",
    "\n",
    "case_id = os.listdir(data_path)\n",
    "\n",
    "paths = [data_path + \"\\\\\" + case for case in case_id][3:5] # All case folders paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(r\"C:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\results\\WSI\\models\\ejemplo_0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da6bea44da14c0bab04f284ab287e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.17 GiB (GPU 0; 12.00 GiB total capacity; 8.01 GiB already allocated; 2.61 GiB free; 8.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\pred_WSI.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000006?line=0'>1</a>\u001b[0m patch_size \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000006?line=2'>3</a>\u001b[0m pred_data(paths, patch_size, [\u001b[39m\"\u001b[39;49m\u001b[39m.svs\u001b[39;49m\u001b[39m\"\u001b[39;49m], net)\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\pred_WSI.ipynb Cell 2'\u001b[0m in \u001b[0;36mpred_data\u001b[1;34m(paths, patch_size, formats, net)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000001?line=12'>13</a>\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000001?line=13'>14</a>\u001b[0m                     label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000001?line=14'>15</a>\u001b[0m                 file_data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred_file(file, patch_size, label, net)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000001?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m file_data\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\pred_WSI.ipynb Cell 3'\u001b[0m in \u001b[0;36mpred_file\u001b[1;34m(file, patch_size, label, net)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000002?line=37'>38</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000002?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(patches))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000002?line=41'>42</a>\u001b[0m acc, loss, output \u001b[39m=\u001b[39m fwd_pass(patches, labels, net)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000002?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m acc, loss, output\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\pred_WSI.ipynb Cell 4'\u001b[0m in \u001b[0;36mfwd_pass\u001b[1;34m(X, y, net, train)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfwd_pass\u001b[39m(X, y, net, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000003?line=1'>2</a>\u001b[0m     loss_function \u001b[39m=\u001b[39m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000003?line=3'>4</a>\u001b[0m     outputs \u001b[39m=\u001b[39m net(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000003?line=5'>6</a>\u001b[0m     matches \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39margmax(i) \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39margmax(j) \u001b[39mfor\u001b[39;00m i,j \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(outputs, y)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alejandro/Desktop/heterogeneous-data/code/WSI/pred_WSI.ipynb#ch0000003?line=7'>8</a>\u001b[0m     acc \u001b[39m=\u001b[39m matches\u001b[39m.\u001b[39mcount(\u001b[39mTrue\u001b[39;00m)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(matches)\n",
      "File \u001b[1;32mc:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torchvision\\models\\resnet.py:283\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=281'>282</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=282'>283</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torchvision\\models\\resnet.py:266\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=263'>264</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=264'>265</a>\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=265'>266</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=266'>267</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torchvision/models/resnet.py?line=267'>268</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Alejandro\\anaconda3\\envs\\openslide\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/Alejandro/anaconda3/envs/openslide/lib/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.17 GiB (GPU 0; 12.00 GiB total capacity; 8.01 GiB already allocated; 2.61 GiB free; 8.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "patch_size = 512\n",
    "\n",
    "pred_data(paths, patch_size, [\".svs\"], net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9420289855072463,\n",
       " tensor(0.1278, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>),\n",
       " [1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.tensor([1,0]).to(\"cuda:0\").float()\n",
    "file = r\"C:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\data\\gdc_download_20220427_144600.480657\\02b370ac-0168-4bd4-8178-54770b134a45\\TCGA-3A-A9IL-01A-01-TS1.F811C83B-1C85-4F69-9E14-E5104E385D84.svs\"\n",
    "patch_size = 512\n",
    "\n",
    "pred_file(file, patch_size, label, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\patch_gen_grid.py:15: DeprecationWarning: Please use `binary_dilation` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  from scipy.ndimage.morphology import binary_dilation, binary_erosion\n",
      "C:\\Users\\Alejandro\\Desktop\\heterogeneous-data\\code\\WSI\\patch_gen_grid.py:15: DeprecationWarning: Please use `binary_erosion` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  from scipy.ndimage.morphology import binary_dilation, binary_erosion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArgumentParser(prog='patch_gen_grid.py', usage=None, description='Generate patches from a given folder of images', formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "%run patch_gen_grid.py --wsi_path=\"C:/Users/Alejandro/Desktop/heterogeneous-data/data/gdc_download_20220427_144600.480657/feba0925-0edd-4077-a5e1-7c93d33283ae\" --patch_path=\"C:/Users/Alejandro/Desktop/heterogeneous-data/data/patches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de74446a7cd3a56c425ecbffb2a7ad915342ddab3c48353acb4566475bd7705f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openslide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
